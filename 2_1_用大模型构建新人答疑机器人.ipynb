{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88e2ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼å…¬å¸å°èœœæ¥å¸®å¿™å•¦~\n",
      "\n",
      "å…³äºé¡¹ç›®ç®¡ç†å·¥å…·çš„é€‰æ‹©ï¼Œä¸»è¦çœ‹å›¢é˜Ÿçš„éœ€æ±‚å’Œé¡¹ç›®çš„å¤æ‚åº¦ã€‚å¸¸ç”¨çš„é¡¹ç›®ç®¡ç†å·¥å…·æœ‰ï¼š\n",
      "\n",
      "1. **Trello** â€” ç®€å•ç›´è§‚ï¼Œé€‚åˆå°å›¢é˜Ÿå’Œè½»é‡çº§é¡¹ç›®ï¼Œç”¨çœ‹æ¿æ–¹å¼ç®¡ç†ä»»åŠ¡ã€‚\n",
      "2. **Asana** â€” åŠŸèƒ½ä¸°å¯Œï¼Œæ”¯æŒä»»åŠ¡åˆ†é…ã€è¿›åº¦è·Ÿè¸ªå’Œæ—¶é—´çº¿è§†å›¾ï¼Œé€‚åˆä¸­å°å‹å›¢é˜Ÿã€‚\n",
      "3. **Jira** â€” ä¸“ä¸šçš„è½¯ä»¶å¼€å‘é¡¹ç›®ç®¡ç†å·¥å…·ï¼Œé€‚åˆæ•æ·å¼€å‘å’Œå¤æ‚é¡¹ç›®ã€‚\n",
      "4. **Microsoft Project** â€” åŠŸèƒ½å¼ºå¤§ï¼Œé€‚åˆå¤§å‹é¡¹ç›®å’Œè¯¦ç»†è®¡åˆ’ç®¡ç†ã€‚\n",
      "5. **é’‰é’‰**æˆ–**ä¼ä¸šå¾®ä¿¡**å†…éƒ¨é›†æˆçš„é¡¹ç›®ç®¡ç†æ¨¡å— â€” æ–¹ä¾¿å†…éƒ¨æ²Ÿé€šå’Œä»»åŠ¡åä½œã€‚\n",
      "\n",
      "å»ºè®®å…ˆæ ¹æ®å›¢é˜Ÿè§„æ¨¡ã€é¡¹ç›®ç±»å‹ä»¥åŠæˆå‘˜ä¹ æƒ¯åšä¸€ä¸ªç®€å•è°ƒç ”ï¼Œå†é€‰æœ€é€‚åˆçš„å·¥å…·ã€‚å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®å¿™æ•´ç†è¯¦ç»†å¯¹æ¯”è¡¨æˆ–ä½¿ç”¨æŒ‡å—å“¦ï¼éœ€è¦å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\")\n",
    ")\n",
    "def get_gpt_response(prompt):\n",
    "    reponse = client.chat.completions.create(\n",
    "        model=os.getenv(\"LLM_MODEL_ID\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": \"ä½ è´Ÿè´£æ•™è‚²å†…å®¹å¼€å‘å…¬å¸çš„ç­”ç–‘ï¼Œä½ çš„åå­—å«å…¬å¸å°èœœï¼Œä½ è¦å›ç­”åŒäº‹ä»¬çš„é—®é¢˜ã€‚\"},\n",
    "                  {\"role\":\"user\",\"content\": prompt}\n",
    "                  ],      \n",
    "    )\n",
    "    return reponse.choices[0].message.content\n",
    "response = get_gpt_response(\"æˆ‘ä»¬å…¬å¸é¡¹ç›®ç®¡ç†åº”è¯¥ä½¿ç”¨ä»€ä¹ˆå·¥å…·\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b97daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‚¨å¥½ï¼Œæˆ‘æ˜¯å…¬å¸å°èœœï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨è§£ç­”ï¼å…³äºæˆ‘ä»¬å…¬å¸çš„é¡¹ç›®ç®¡ç†å·¥å…·é€‰æ‹©ï¼Œå»ºè®®è€ƒè™‘ä»¥ä¸‹å‡ æ¬¾å¸¸ç”¨ä¸”é€‚åˆæ•™è‚²å†…å®¹å¼€å‘è¡Œä¸šçš„å·¥å…·ï¼š\n",
      "\n",
      "1. **é£ä¹¦ï¼ˆLarkï¼‰**  \n",
      "   - é›†æˆäº†å³æ—¶é€šè®¯ã€æ–‡æ¡£åä½œã€ä»»åŠ¡ç®¡ç†äºä¸€ä½“ï¼Œæ–¹ä¾¿å›¢é˜Ÿæ²Ÿé€šå’Œèµ„æ–™å…±äº«ã€‚  \n",
      "   - æ”¯æŒä»»åŠ¡åˆ†é…ã€è¿›åº¦è·Ÿè¸ªï¼Œé€‚åˆè·¨éƒ¨é—¨åä½œã€‚\n",
      "\n",
      "2. **Teambition**  \n",
      "   - ä¸“æ³¨é¡¹ç›®å’Œä»»åŠ¡ç®¡ç†ï¼Œæ”¯æŒç”˜ç‰¹å›¾ã€çœ‹æ¿æ¨¡å¼ï¼Œå¸®åŠ©ç›´è§‚ç®¡ç†é¡¹ç›®è¿›åº¦ã€‚  \n",
      "   - æ”¯æŒæ–‡æ¡£åœ¨çº¿åä½œå’Œå®æ—¶è®¨è®ºã€‚\n",
      "\n",
      "3. **Trello**  \n",
      "   - çœ‹æ¿å¼ä»»åŠ¡ç®¡ç†å·¥å…·ï¼Œç®€å•ç›´è§‚ï¼Œæ–¹ä¾¿æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€å’Œä¼˜å…ˆçº§ã€‚  \n",
      "   - æ”¯æŒæ’ä»¶æ‰©å±•å’Œç§»åŠ¨ç«¯ä½¿ç”¨ã€‚\n",
      "\n",
      "4. **Asana**  \n",
      "   - é€‚åˆå¤æ‚é¡¹ç›®ç®¡ç†ï¼Œæ”¯æŒå¤šå±‚çº§ä»»åŠ¡æ‹†åˆ†åŠæ—¶é—´çº¿è§†å›¾ã€‚  \n",
      "   - å…·å¤‡æé†’ã€ä»»åŠ¡ä¾èµ–ç­‰åŠŸèƒ½ï¼Œæé«˜é¡¹ç›®æ‰§è¡Œæ•ˆç‡ã€‚\n",
      "\n",
      "æ ¹æ®æˆ‘ä»¬å…¬å¸çš„å›¢é˜Ÿè§„æ¨¡å’Œå·¥ä½œä¹ æƒ¯ï¼Œå¦‚æœä¹ æƒ¯ä½¿ç”¨é£ä¹¦åŠå…¬å¥—ä»¶ï¼Œå¯ä»¥ä¼˜å…ˆè€ƒè™‘é£ä¹¦è‡ªå¸¦çš„ä»»åŠ¡å’Œé¡¹ç›®ç®¡ç†åŠŸèƒ½ï¼›å¦‚æœå¸Œæœ›æ›´ä¸“æ³¨äºé¡¹ç›®è¿›åº¦å’Œä»»åŠ¡åˆ†é…ï¼Œä¹Ÿå¯ä»¥ç»“åˆTeambitionæˆ–Trelloä½¿ç”¨ã€‚\n",
      "\n",
      "æ‚¨å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚ï¼Œä¾‹å¦‚å›¢é˜Ÿåä½œæ¨¡å¼ã€é¡¹ç›®å¤æ‚åº¦å’Œä½¿ç”¨ä¹ æƒ¯ï¼Œé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚å¦‚éœ€ï¼Œæˆ‘å¯ä»¥å¸®å¿™å‡†å¤‡ä½¿ç”¨æŒ‡å—æˆ–åŸ¹è®­èµ„æ–™å“¦ï¼æœ‰å…¶å®ƒç›¸å…³é—®é¢˜ä¹Ÿæ¬¢è¿éšæ—¶é—®æˆ‘ã€‚NoneNone"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\")\n",
    ")\n",
    "def get_gpt_stream_response(user_prompt, system_prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"LLM_MODEL_ID\"),\n",
    "        messages=[{\"role\": \"system\", \"content\":user_prompt},\n",
    "                  {\"role\":\"user\",\"content\": system_prompt}\n",
    "                  ],\n",
    "        stream=True   \n",
    "    )\n",
    "    for chunk in response:\n",
    "        yield chunk.choices[0].delta.content\n",
    "response = get_gpt_stream_response(user_prompt=\"æˆ‘ä»¬å…¬å¸é¡¹ç›®ç®¡ç†åº”è¯¥ç”¨ä»€ä¹ˆå·¥å…·\",system_prompt=\"ä½ è´Ÿè´£æ•™è‚²å†…å®¹å¼€å‘å…¬å¸çš„ç­”ç–‘ï¼Œä½ çš„åå­—å«å…¬å¸å°èœœï¼Œä½ è¦å›ç­”åŒäº‹ä»¬çš„é—®é¢˜ã€‚\")\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76afa3",
   "metadata": {},
   "source": [
    "ğŸ§  èƒŒæ™¯çŸ¥è¯†ï¼šä»€ä¹ˆæ˜¯ yieldï¼Ÿ\n",
    "- yield æ˜¯ Python ä¸­ç”¨äºå®šä¹‰ ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰ çš„å…³é”®å­—ã€‚\n",
    "- ä¸ return ä¸åŒï¼Œyield åœ¨å‡½æ•°ä¸­æ¯æ¬¡é‡åˆ°æ—¶ä¼š æš‚åœæ‰§è¡Œå¹¶è¿”å›ä¸€ä¸ªå€¼ï¼Œä¸‹æ¬¡è°ƒç”¨æ—¶ä»ä¸Šæ¬¡æš‚åœçš„åœ°æ–¹ç»§ç»­ã€‚\n",
    "- è¿™ä½¿å¾—å‡½æ•°å¯ä»¥ é€æ­¥è¿”å›ç»“æœï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰å†…å®¹â€”â€”éå¸¸é€‚åˆå¤„ç†â€œæµå¼å“åº”â€ï¼ˆstreamingï¼‰\n",
    "\n",
    "- chunk.choicesï¼šä¸€ä¸ªåˆ—è¡¨ï¼Œé€šå¸¸åªæœ‰ä¸€ä¸ªé€‰é¡¹ï¼ˆç´¢å¼• 0ï¼‰ï¼Œå› ä¸ºé»˜è®¤åªç”Ÿæˆä¸€ä¸ªå›ç­”ã€‚\n",
    "- .deltaï¼šè¡¨ç¤ºâ€œå¢é‡â€ï¼Œå³è¿™æ¬¡æ–°å¢çš„å†…å®¹ï¼ˆä¸æ˜¯å®Œæ•´å›ç­”ï¼Œè€Œæ˜¯æ–°å¢çš„éƒ¨åˆ†ï¼‰ã€‚\n",
    "- .contentï¼šè¿™æ¬¡å¢é‡ä¸­çš„å®é™…æ–‡æœ¬å†…å®¹ï¼ˆå¯èƒ½æ˜¯å‡ ä¸ªå­—ã€ä¸€ä¸ªè¯ï¼Œç”šè‡³ Noneï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba691c4",
   "metadata": {},
   "source": [
    "è®©ä¸Šè¿°ä»£ç æ›´å¥å£®ï¼Œé¿å…Noneè¢«æ‰“å°å‡ºæ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b397f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼Œæˆ‘æ˜¯å…¬å¸å°èœœï¼å…³äºæˆ‘ä»¬å…¬å¸é¡¹ç›®ç®¡ç†ç”¨ä»€ä¹ˆå·¥å…·ï¼Œè¿™å–å†³äºé¡¹ç›®çš„è§„æ¨¡ã€å›¢é˜Ÿåä½œéœ€æ±‚ä»¥åŠå…·ä½“ä¸šåŠ¡æµç¨‹ã€‚å¸¸è§ä¸”é€‚åˆæ•™è‚²å†…å®¹å¼€å‘å…¬å¸çš„é¡¹ç›®ç®¡ç†å·¥å…·æœ‰ï¼š\n",
      "\n",
      "1. **Trello**  \n",
      "   - ç®€å•ç›´è§‚ï¼Œé€‚åˆä»»åŠ¡çœ‹æ¿ç®¡ç†  \n",
      "   - é€‚åˆå†…å®¹å¼€å‘æµç¨‹çš„åˆ†é˜¶æ®µç®¡ç†ï¼Œå¦‚ç­–åˆ’ã€åˆ¶ä½œã€å®¡æ ¸ã€å‘å¸ƒ  \n",
      "\n",
      "2. **Asana**  \n",
      "   - åŠŸèƒ½å…¨é¢ï¼Œæ”¯æŒä»»åŠ¡åˆ†é…ã€è¿›åº¦è·Ÿè¸ªå’Œæ—¶é—´ç®¡ç†  \n",
      "   - é€‚åˆå¤šäººåä½œå’Œå¤æ‚é¡¹ç›®çš„ç»†è‡´ç®¡ç†  \n",
      "\n",
      "3. **Jira**  \n",
      "   - æ›´é€‚åˆè½¯ä»¶å¼€å‘é¡¹ç›®ï¼Œä½†å¦‚æœå†…å®¹å¼€å‘éœ€è¦è·Ÿè¸ªbugå’Œéœ€æ±‚ï¼Œä¹Ÿèƒ½ä½¿ç”¨  \n",
      "   - çµæ´»çš„å·¥ä½œæµç¨‹é…ç½®  \n",
      "\n",
      "4. **Microsoft Teams + Planner**  \n",
      "   - å¦‚æœå…¬å¸å·²ç»ä½¿ç”¨Office 365ï¼Œç»“åˆTeamsæ²Ÿé€šå’ŒPlannerä»»åŠ¡ç®¡ç†ï¼Œæ•ˆç‡é«˜ä¸”é›†æˆåº¦å¥½  \n",
      "\n",
      "5. **ClickUp**  \n",
      "   - é›†ä»»åŠ¡ç®¡ç†ã€æ–‡æ¡£ã€æ—¶é—´è·Ÿè¸ªäºä¸€ä½“ï¼ŒåŠŸèƒ½ä¸°å¯Œä¸”è‡ªå®šä¹‰å¼º  \n",
      "\n",
      "å»ºè®®å…ˆæ ¹æ®å›¢é˜Ÿä¹ æƒ¯å’Œé¡¹ç›®éœ€æ±‚è¯•ç”¨ä¸€ä¸¤æ¬¾å·¥å…·ï¼Œæ‰¾åˆ°æœ€é€‚åˆå¤§å®¶çš„ã€‚å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®å¿™æ•´ç†å…·ä½“å¯¹æ¯”è¡¨å“¦ï¼ä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ ä»¬å›¢é˜Ÿçš„å…·ä½“éœ€æ±‚å—ï¼Ÿæ¯”å¦‚å›¢é˜Ÿè§„æ¨¡ã€é¡¹ç›®å¤æ‚åº¦ç­‰ï¼Œæˆ‘å¸®ä½ æ¨èæ›´åˆé€‚çš„æ–¹æ¡ˆã€‚"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"LLM_BASE_URL\")\n",
    ")\n",
    "def get_gpt_stream_response(user_prompt, system_prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"LLM_MODEL_ID\"),\n",
    "        messages=[{\"role\": \"system\", \"content\":user_prompt},\n",
    "                  {\"role\":\"user\",\"content\": system_prompt}\n",
    "                  ],\n",
    "        stream=True   \n",
    "    )\n",
    "    for chunk in response:\n",
    "        yield chunk.choices[0].delta.content\n",
    "response = get_gpt_stream_response(user_prompt=\"æˆ‘ä»¬å…¬å¸é¡¹ç›®ç®¡ç†åº”è¯¥ç”¨ä»€ä¹ˆå·¥å…·\",system_prompt=\"ä½ è´Ÿè´£æ•™è‚²å†…å®¹å¼€å‘å…¬å¸çš„ç­”ç–‘ï¼Œä½ çš„åå­—å«å…¬å¸å°èœœï¼Œä½ è¦å›ç­”åŒäº‹ä»¬çš„é—®é¢˜ã€‚\")\n",
    "for chunk in response:\n",
    "    if chunk is not None:\n",
    "        print(chunk, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f048b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n",
      "è¾“å‡º[i + 1]éªé©¬\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_gpt_stream_response(user_prompt, system_prompt, temperature, top_p):\n",
    "    response = client.chat.completions.create(\n",
    "        model=os.getenv(\"LLM_MODEL_ID\"),\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\": system_prompt},\n",
    "            {\"role\":\"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in response:\n",
    "        yield chunk.choices[0].delta.content\n",
    "        \n",
    "def print_gpt_stream_response(user_prompt, system_prompt, temperature=0.7, top_p=0.8, iterations=10):\n",
    "    for i in range(iterations):\n",
    "        print(f\"è¾“å‡º{i + 1}\", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "        response = get_gpt_stream_response(user_prompt, system_prompt,temperature,top_p)\n",
    "        output_content = \"\"\n",
    "        for chunk in response:\n",
    "            if chunk is not None:\n",
    "                output_content += chunk\n",
    "        print(output_content)\n",
    "        \n",
    "print_gpt_stream_response(user_prompt=\"é©¬ä¹Ÿå¯ä»¥å«åš\", system_prompt=\"è¯·å¸®æˆ‘ç»­å†™å†…å®¹ï¼Œå­—æ•°è¦æ±‚æ˜¯4ä¸ªæ±‰å­—ä»¥å†…ã€‚\", temperature=0)\n",
    "print_gpt_stream_response(user_prompt=\"é©¬ä¹Ÿå¯ä»¥å«åš\", system_prompt=\"è¯·å¸®æˆ‘ç»­å†™å†…å®¹ï¼Œå­—æ•°è¦æ±‚æ˜¯4ä¸ªæ±‰å­—ä»¥å†…ã€‚\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37690b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼Œè½¯ä»¶ä¸€ç»„ç›®å‰æ­£åœ¨ä½¿ç”¨Jiraè¿›è¡Œé¡¹ç›®ç®¡ç†ã€‚æ ¹æ®å…¬å¸çš„è®¡åˆ’ï¼Œè½¯ä»¶ç ”å‘ä¸€ç»„å°†åœ¨2026å¹´ä¹‹å‰é€æ­¥åˆ‡æ¢åˆ°Microsoft Projectã€‚æ‰€ä»¥ä½ ç°åœ¨å¯ä»¥ç»§ç»­ä½¿ç”¨Jiraï¼Œåç»­ä¼šæœ‰è¿‡æ¸¡åˆ°Microsoft Projectçš„å®‰æ’å“¦ã€‚å¦‚æœ‰éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ äº†è§£åˆ‡æ¢çš„å…·ä½“æ—¶é—´å’ŒåŸ¹è®­ä¿¡æ¯ã€‚NoneNone"
     ]
    }
   ],
   "source": [
    "user_question = \"æˆ‘æ˜¯è½¯ä»¶ä¸€ç»„çš„ï¼Œè¯·é—®é¡¹ç›®ç®¡ç†åº”è¯¥ç”¨ä»€ä¹ˆå·¥å…·\"\n",
    "\n",
    "knowledge = \"\"\"å…¬å¸é¡¹ç›®ç®¡ç†å·¥å…·æœ‰ä¸¤ç§é€‰æ‹©ï¼š\n",
    "  1. **Jira**ï¼šå¯¹äºè½¯ä»¶å¼€å‘å›¢é˜Ÿæ¥è¯´ï¼ŒJira æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œæ”¯æŒæ•æ·å¼€å‘æ–¹æ³•ï¼Œå¦‚Scrumå’ŒKanbanã€‚å®ƒæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬é—®é¢˜è·Ÿè¸ªã€æ—¶é—´è·Ÿè¸ªç­‰ã€‚\n",
    "\n",
    "  2. **Microsoft Project**ï¼šå¯¹äºå¤§å‹ä¼ä¸šæˆ–å¤æ‚é¡¹ç›®ï¼ŒMicrosoft Project æä¾›äº†è¯¦ç»†çš„è®¡åˆ’åˆ¶å®šã€èµ„æºåˆ†é…å’Œæˆæœ¬æ§åˆ¶ç­‰åŠŸèƒ½ã€‚å®ƒæ›´é€‚åˆé‚£äº›éœ€è¦ä¸¥æ ¼æ§åˆ¶é¡¹ç›®æ—¶é—´å’Œæˆæœ¬çš„åœºæ™¯ã€‚\n",
    "  \n",
    "  åœ¨ä¸€èˆ¬æƒ…å†µä¸‹è¯·ä½¿ç”¨Microsoft Projectï¼Œå…¬å¸è´­ä¹°äº†å®Œæ•´çš„è®¸å¯è¯ã€‚è½¯ä»¶ç ”å‘ä¸€ç»„ã€ä¸‰ç»„å’Œå››ç»„æ­£åœ¨ä½¿ç”¨Jiraï¼Œè®¡åˆ’äº2026å¹´ä¹‹å‰é€æ­¥åˆ‡æ¢è‡³Microsoft Projectã€‚\n",
    "\"\"\"\n",
    "\n",
    "response = get_gpt_stream_response(\n",
    "    user_prompt=user_question,\n",
    "    # å°†å…¬å¸é¡¹ç›®ç®¡ç†å·¥å…·ç›¸å…³çš„çŸ¥è¯†ä½œä¸ºèƒŒæ™¯ä¿¡æ¯ä¼ å…¥ç³»ç»Ÿæç¤ºè¯\n",
    "    system_prompt=\"ä½ è´Ÿè´£æ•™è‚²å†…å®¹å¼€å‘å…¬å¸çš„ç­”ç–‘ï¼Œä½ çš„åå­—å«å…¬å¸å°èœœï¼Œä½ è¦å›ç­”å­¦å‘˜çš„é—®é¢˜ã€‚\"+ knowledge,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studyE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
